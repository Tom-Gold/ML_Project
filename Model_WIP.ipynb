{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sklearn import preprocessing \n",
    "import glob\n",
    "import matplotlib\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tempfile\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "import sklearn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "COLUMNS = ['AB', 'CS', 'EP', 'G', 'LC', 'P', 'VBAC', 'Age', 'amniotic_fluid', 'Position', 'weight', 'week', 'induced', 'target']\n",
    "NUM_COLUMNS = ['AB', 'CS', 'EP', 'G', 'LC', 'P', 'VBAC', 'Age', 'weight', 'week']\n",
    "CAT_COLUMNS = ['amniotic_fluid', 'Position', 'induced']\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.rcParams['figure.figsize'] = (12, 10)\n",
    "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.getcwd()\n",
    "print(path)\n",
    "dataset_path = path + \"/dataset\"\n",
    "# dataset_path = path + \"\\\\dataset\"\n",
    "print(dataset_path)\n",
    "file_list = glob.glob(dataset_path + \"/*.csv\")\n",
    "print(file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = []\n",
    "seis = []\n",
    "outp = pd.DataFrame()\n",
    "logd = pd.DataFrame()\n",
    "for file in file_list:\n",
    "    df = pd.read_csv(file)\n",
    "    df.fillna(0, inplace=True)\n",
    "    df = convetr_to_num(df)\n",
    "    train_df, test_df = train_test_split(df, test_size=0.2)\n",
    "    train_df, val_df = train_test_split(train_df, test_size=0.2)\n",
    "    # Form np arrays of labels and features.\n",
    "    train_labels = np.array(train_df.pop('target'))\n",
    "    bool_train_labels = train_labels != 0\n",
    "    val_labels = np.array(val_df.pop('target'))\n",
    "    test_labels = np.array(test_df.pop('target'))\n",
    "\n",
    "    train_features = np.array(train_df)\n",
    "    val_features = np.array(val_df)\n",
    "    test_features = np.array(test_df)\n",
    "    \n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "                                                monitor='val_auc', \n",
    "                                                verbose=1,\n",
    "                                                patience=10,\n",
    "                                                mode='max',\n",
    "                                                restore_best_weights=True)\n",
    "#     model = make_model_mlp()\n",
    "    model = make_model()\n",
    "    \n",
    "    baseline_history = model.fit(train_features,\n",
    "                                 train_labels,\n",
    "                                 batch_size=BATCH_SIZE,\n",
    "                                 epochs=EPOCHS,\n",
    "                                 callbacks = [early_stopping],\n",
    "                                 validation_data=(val_features, val_labels))\n",
    "    \n",
    "    plot_metrics(baseline_history, file[:-4])\n",
    "    \n",
    "    train_predictions_baseline = model.predict(train_features, batch_size=BATCH_SIZE)\n",
    "    test_predictions_baseline = model.predict(test_features, batch_size=BATCH_SIZE)\n",
    "    \n",
    "    plot_cm(test_labels, test_predictions_baseline, file[:-4] + \"cm\")\n",
    "\n",
    "    baseline_results = model.evaluate(test_features, test_labels, batch_size=BATCH_SIZE, verbose=0)\n",
    "    logdtemp = pd.DataFrame(baseline_results, columns=[file.split('/')[-1][:-4]])\n",
    "    logd = pd.concat([logd, logdtemp.T])\n",
    "logd.to_csv(\"summary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = file_list[0]\n",
    "file.split('/')[-1][:-4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Form np arrays of labels and features.\n",
    "train_labels = np.array(train_df.pop('target'))\n",
    "bool_train_labels = train_labels != 0\n",
    "val_labels = np.array(val_df.pop('target'))\n",
    "test_labels = np.array(test_df.pop('target'))\n",
    "\n",
    "train_features = np.array(train_df)\n",
    "val_features = np.array(val_df)\n",
    "test_features = np.array(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "METRICS = [\n",
    "      keras.metrics.TruePositives(name='tp'),\n",
    "      keras.metrics.FalsePositives(name='fp'),\n",
    "      keras.metrics.TrueNegatives(name='tn'),\n",
    "      keras.metrics.FalseNegatives(name='fn'), \n",
    "      keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "      keras.metrics.Precision(name='precision'),\n",
    "      keras.metrics.Recall(name='recall'),\n",
    "      keras.metrics.AUC(name='auc'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model_mlp(metrics = METRICS, output_bias=None):\n",
    "    if output_bias is not None:\n",
    "        output_bias = tf.keras.initializers.Constant(output_bias)\n",
    "        \n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Dense( 32, activation='relu',input_shape=(train_features.shape[-1],)),\n",
    "        keras.layers.Dense( 64, activation='relu'),\n",
    "        keras.layers.Dense(1, activation='sigmoid', bias_initializer=output_bias),\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(lr=1e-3),\n",
    "        loss=keras.losses.BinaryCrossentropy(),\n",
    "        metrics=metrics)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(metrics = METRICS, output_bias=None):\n",
    "    if output_bias is not None:\n",
    "        output_bias = tf.keras.initializers.Constant(output_bias)\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Dense(t32, activation='relu',input_shape=(train_features.shape[-1],)),\n",
    "        keras.layers.Dropout(0.1),\n",
    "        keras.layers.Dense(1, activation='sigmoid', bias_initializer=output_bias),\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer=keras.optimizers.Adam(lr=1e-3), loss=keras.losses.BinaryCrossentropy(),metrics=metrics)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_df = pd.DataFrame(train_features[ bool_train_labels], columns = train_df.columns)\n",
    "neg_df = pd.DataFrame(train_features[~bool_train_labels], columns = train_df.columns)\n",
    "\n",
    "sns.jointplot(pos_df['CS'], pos_df['P'],\n",
    "              kind='hex', xlim = (-5,5), ylim = (-5,5))\n",
    "plt.suptitle(\"Positive distribution\")\n",
    "\n",
    "sns.jointplot(neg_df['CS'], neg_df['P'],\n",
    "              kind='hex', xlim = (-5,5), ylim = (-5,5))\n",
    "_ = plt.suptitle(\"Negative distribution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg, pos = np.bincount(df['target'])\n",
    "total = neg + pos\n",
    "print('Examples:\\n    Total: {}\\n    Positive: {} ({:.2f}% of total)\\n'.format(\n",
    "    total, pos, 100 * pos / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_bias = np.log([pos/neg])\n",
    "initial_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_model(output_bias = initial_bias)\n",
    "model.predict(train_features[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(history, label, n):\n",
    "  # Use a log scale to show the wide range of values.\n",
    "    plt.semilogy(history.epoch,  history.history['loss'],\n",
    "               color=colors[n], label='Train '+label)\n",
    "    plt.semilogy(history.epoch,  history.history['val_loss'],\n",
    "          color=colors[n], label='Val '+label,\n",
    "          linestyle=\"--\")\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_bias_history = model.fit(\n",
    "    train_features,\n",
    "    train_labels,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=20,\n",
    "    validation_data=(val_features, val_labels), \n",
    "    verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "careful_bias_history = model.fit(\n",
    "    train_features,\n",
    "    train_labels,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=20,\n",
    "    validation_data=(val_features, val_labels), \n",
    "    verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(zero_bias_history, \"Zero Bias\", 0)\n",
    "plot_loss(careful_bias_history, \"Careful Bias\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_weights = os.path.join(tempfile.mkdtemp(),'initial_weights')\n",
    "model.save_weights(initial_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_model()\n",
    "model.load_weights(initial_weights)\n",
    "baseline_history = model.fit(\n",
    "    train_features,\n",
    "    train_labels,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks = [early_stopping],\n",
    "    validation_data=(val_features, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(history, file):\n",
    "    metrics =  ['loss', 'auc', 'precision', 'recall']\n",
    "    for n, metric in enumerate(metrics):\n",
    "        name = metric.replace(\"_\",\" \").capitalize()\n",
    "        plt.subplot(2,2,n+1)\n",
    "        plt.plot(history.epoch,  history.history[metric], color=colors[0], label='Train')\n",
    "        plt.plot(history.epoch, history.history['val_'+metric],\n",
    "                 color=colors[0], linestyle=\"--\", label='Val')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel(name)\n",
    "        if metric == 'loss':\n",
    "            plt.ylim([0, plt.ylim()[1]])\n",
    "        elif metric == 'auc':\n",
    "            plt.ylim([0.8,1])\n",
    "        else:\n",
    "            plt.ylim([0,1])\n",
    "\n",
    "        plt.legend()\n",
    "    plt.savefig(file + '.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(baseline_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predictions_baseline = model.predict(train_features, batch_size=BATCH_SIZE)\n",
    "test_predictions_baseline = model.predict(test_features, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cm(labels, predictions, file, p=0.5):\n",
    "    cm = confusion_matrix(labels, predictions > p)\n",
    "    plt.figure(figsize=(5,5))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\")\n",
    "    plt.title('Confusion matrix @{:.2f}'.format(p))\n",
    "    plt.ylabel('Actual label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.savefig(file + '.png')\n",
    "    plt.close()\n",
    "#     print('Legitimate Transactions Detected (True Negatives): ', cm[0][0])\n",
    "#     print('Legitimate Transactions Incorrectly Detected (False Positives): ', cm[0][1])\n",
    "#     print('Fraudulent Transactions Missed (False Negatives): ', cm[1][0])\n",
    "#     print('Fraudulent Transactions Detected (True Positives): ', cm[1][1])\n",
    "#     print('Total Fraudulent Transactions: ', np.sum(cm[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_results = model.evaluate(test_features, test_labels,\n",
    "                                  batch_size=BATCH_SIZE, verbose=0)\n",
    "for name, value in zip(model.metrics_names, baseline_results):\n",
    "    print(name, ': ', value)\n",
    "print()\n",
    "\n",
    "plot_cm(test_labels, test_predictions_baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc(name, labels, predictions,file, **kwargs):\n",
    "    fp, tp, _ = sklearn.metrics.roc_curve(labels, predictions)\n",
    "\n",
    "    plt.plot(100*fp, 100*tp, label=name, linewidth=2, **kwargs)\n",
    "    plt.xlabel('False positives [%]')\n",
    "    plt.ylabel('True positives [%]')\n",
    "    plt.xlim([-0.5,20])\n",
    "    plt.ylim([80,100.5])\n",
    "    plt.grid(True)\n",
    "    ax = plt.gca()\n",
    "    ax.set_aspect('equal')\n",
    "    plt.savefig(file + '.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc(\"Train Baseline\", train_labels, train_predictions_baseline, color=colors[0])\n",
    "plot_roc(\"Test Baseline\", test_labels, test_predictions_baseline, color=colors[0], linestyle='--')\n",
    "plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling by total/2 helps keep the loss to a similar magnitude.\n",
    "# The sum of the weights of all examples stays the same.\n",
    "weight_for_0 = (1 / neg)*(total)/2.0 \n",
    "weight_for_1 = (1 / pos)*(total)/2.0\n",
    "\n",
    "class_weight = {0: weight_for_0, 1: weight_for_1}\n",
    "\n",
    "print('Weight for class 0: {:.2f}'.format(weight_for_0))\n",
    "print('Weight for class 1: {:.2f}'.format(weight_for_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_model = make_model()\n",
    "weighted_model.load_weights(initial_weights)\n",
    "\n",
    "weighted_history = weighted_model.fit(\n",
    "    train_features,\n",
    "    train_labels,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks = [early_stopping],\n",
    "    validation_data=(val_features, val_labels),\n",
    "    # The class weights go here\n",
    "    class_weight=class_weight) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predictions_weighted = weighted_model.predict(train_features, batch_size=BATCH_SIZE)\n",
    "test_predictions_weighted = weighted_model.predict(test_features, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_results = weighted_model.evaluate(test_features, test_labels,\n",
    "                                           batch_size=BATCH_SIZE, verbose=0)\n",
    "for name, value in zip(weighted_model.metrics_names, weighted_results):\n",
    "    print(name, ': ', value)\n",
    "print()\n",
    "\n",
    "plot_cm(test_labels, test_predictions_weighted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc(\"Train Baseline\", train_labels, train_predictions_baseline, color=colors[0])\n",
    "plot_roc(\"Test Baseline\", test_labels, test_predictions_baseline, color=colors[0], linestyle='--')\n",
    "\n",
    "plot_roc(\"Train Weighted\", train_labels, train_predictions_weighted, color=colors[1])\n",
    "plot_roc(\"Test Weighted\", test_labels, test_predictions_weighted, color=colors[1], linestyle='--')\n",
    "\n",
    "\n",
    "plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset(raw_df: pd.DataFrame, dataset_guide: pd.DataFrame):\n",
    "    for ind, data_num, one_hot, bucket, scal, syn, resam in dataset_guide.itertuples():\n",
    "        filen = \"\\\\data\"\n",
    "        df_path = dataset_path + \"\\\\data\"\n",
    "        df = raw_df.copy()\n",
    "#         df_y = df.pop('target')\n",
    "        if bucket:\n",
    "            df = df_to_bucket(df)\n",
    "            df_path += \"_bucket\"\n",
    "            filen += \"_bucket\"\n",
    "        if scal:\n",
    "            df = df_to_scal(df)\n",
    "            df_path += \"_scal\"\n",
    "            filen += \"_scal\"\n",
    "        if syn:\n",
    "            df = df_to_syn(df)\n",
    "            df_path += \"_syn\"\n",
    "            filen += \"_syn\"\n",
    "        if resam:\n",
    "            df = df_to_resam(df)\n",
    "            df_path += \"_resam\"\n",
    "            filen += \"_resam\"\n",
    "        if one_hot:\n",
    "            df = df_to_one_hot(df, bucket)\n",
    "            df_path += \"_one_hot\"\n",
    "            filen += \"_one_hot\"\n",
    "        os.mkdir(df_path)\n",
    "#         df['target'] = df_y\n",
    "        df.to_csv(df_path + \"\\\\data.csv\", index=False)\n",
    "        df.to_csv(dataset_path + filen + \".csv\", index=False)\n",
    "#         df.\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_one_hot(df, bucket):\n",
    "    df_y = df.pop('target')\n",
    "    ONE_HOT_COLS = ['amniotic_fluid', 'Position']\n",
    "    columns_to_one_hot = ONE_HOT_COLS+NUM_COLUMNS if bucket else ONE_HOT_COLS \n",
    "    df_one_hot = pd.get_dummies(df[columns_to_one_hot].astype(str))\n",
    "    df.drop(columns=columns_to_one_hot, inplace=True)\n",
    "    df = pd.concat([df, df_one_hot], axis=1)\n",
    "    df['target'] = df_y\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_bucket(df, n_bins=5):\n",
    "    df_y = df.pop('target')\n",
    "    for col in NUM_COLUMNS:\n",
    "        df[col] = pd.cut(df[col], bins=n_bins, labels=list(range(n_bins)))\n",
    "    df['target'] = df_y\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_scal(df):\n",
    "    df_y = df.pop('target')\n",
    "    scaler = preprocessing.MinMaxScaler()\n",
    "    scaled_df = scaler.fit_transform(df[NUM_COLUMNS])\n",
    "    df[NUM_COLUMNS] = pd.DataFrame(scaled_df, columns=NUM_COLUMNS)\n",
    "    df['target'] = df_y\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_syn(df):\n",
    "    df_y = df.pop('target')\n",
    "    for col in CAT_COLUMNS:\n",
    "        uniam = df[col].unique().tolist()\n",
    "        val = list(range(len(uniam)))\n",
    "        df[col] = df[col].replace(to_replace=uniam, value=val)\n",
    "    smote = SMOTE(sampling_strategy='minority')\n",
    "    X_sm, y_sm = smote.fit_sample(df, df_y)\n",
    "    df = pd.concat([X_sm, y_sm], axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_resam(df):\n",
    "    # Class count\n",
    "    count_class_0, count_class_1 = df.target.value_counts()\n",
    "\n",
    "    # Divide by class\n",
    "    df_class_0 = df[df['target'] == 0]\n",
    "    df_class_1 = df[df['target'] == 1]\n",
    "    df_class_1_over = df_class_1.sample(count_class_0, replace=True)\n",
    "    df_test_over = pd.concat([df_class_0, df_class_1_over], axis=0)\n",
    "\n",
    "    print('Random over-sampling:')\n",
    "    print(df_test_over.target.value_counts())\n",
    "\n",
    "    df_test_over.target.value_counts().plot(kind='bar', title='Count (target)');\n",
    "    return df_test_over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convetr_to_num(df):\n",
    "    for col in df.columns:\n",
    "        uniam = df[col].unique().tolist()\n",
    "        val = list(range(len(uniam)))\n",
    "        df[col] = df[col].replace(to_replace=uniam, value=val)\n",
    "    return df\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_correlations(df, feat_amount=10, label='target'):\n",
    "#     global CAT_COLUMNS\n",
    "#     for col in CAT_COLUMNS:\n",
    "#         df[col] = df[col].astype('category').cat.codes\n",
    "#     df[CAT_COLUMNS] = df[CAT_COLUMNS].astype('category').cat.codes\n",
    "#     result_df_corr = df.drop(columns=[label]).corrwith(df[label])\n",
    "#     df.drop(columns=['Loan ID']).corrwith(df['Loan Status'])\n",
    "    result_df_corr = df.corr()\n",
    "    result_df_corr = abs(result_df_corr[label])\n",
    "    print(result_df_corr[label])\n",
    "    print(result_df_corr)\n",
    "    result_df_corr = result_df_corr.sort_values(ascending=False)\n",
    "    result_df_corr = result_df_corr[1:feat_amount]\n",
    "#     result = result_df_corr.index.tolist()\n",
    "    return result_df_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_K_Best(_x, y, K=10):\n",
    "    #apply SelectKBest class to extract top K best features\n",
    "    cols = [col for col in _x.columns if _x[col].dtype != 'object']\n",
    "    x = _x[cols].copy()\n",
    "    best_features = SelectKBest(score_func=chi2, k=K)\n",
    "    fit = best_features.fit(x, y)\n",
    "    dfscores = pd.DataFrame(fit.scores_)\n",
    "    dfcolumns = pd.DataFrame(x.columns)\n",
    "    #concat two dataframes for better visualization \n",
    "    featureScores = pd.concat([dfcolumns,dfscores],axis=1)\n",
    "    featureScores.columns = ['Featues','Score']  #naming the dataframe columns\n",
    "    feat = featureScores.nlargest(K,'Score')\n",
    "    print(feat)  #print K best features\n",
    "#     res = feat['Featues'].values.tolist()\n",
    "    return feat"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
